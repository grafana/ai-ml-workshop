{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "# from src.flair_ner import flair_batch_predict, save_as_flair_fmt, load_flair_corpus, train_flair_ner, load_flair_ner\n",
    "\n",
    "with open(\"data/train.json\") as f:\n",
    "    train = json.load(f)\n",
    "\n",
    "with open(\"data/test.json\") as f:\n",
    "    test = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random model\n",
    "\n",
    "It's always a good idea to start with a non-sense model to make sure evaluation is working and set a baseline.\n",
    "\n",
    "If our approaches have similar results to the non-sense model, then something might be wrong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from src.utils import extract_spans, evaluate_ner\n",
    "\n",
    "labels_to_predict = ['O', 'B-SVC', 'B-ENV']\n",
    "\n",
    "random.seed(42)\n",
    "random_predictions = [\n",
    "    {\n",
    "        \"token\": data['tokens'],\n",
    "        \"labels\": extract_spans([random.choice(labels_to_predict) for _ in range(len(data['tokens']))])\n",
    "    }\n",
    "    for data in test\n",
    "]\n",
    "\n",
    "random_predictions[:2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_score = evaluate_ner(test, random_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regex model\n",
    "\n",
    "Simple model like regex or using heuristic can work surprisingly well sometimes. Especially in cases where we don't have sufficient data.\n",
    "\n",
    "In this example, we don't actually need a training data since we should be able to just generate a list of services and environments we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.regex_ner import RegexModel\n",
    "\n",
    "# get list of services and environments\n",
    "services = set()\n",
    "environments = set()\n",
    "\n",
    "for data in train:\n",
    "    for span in data['labels']:\n",
    "        span_text = \" \".join(data['tokens'][span[0]:span[1]]) \n",
    "        if span[2] == 'SVC':\n",
    "            services.add(span_text)\n",
    "        elif span[2] == 'ENV':\n",
    "            environments.add(span_text)\n",
    "\n",
    "services, environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "regex_model = RegexModel()\n",
    "regex_model.fit(services, environments)\n",
    "\n",
    "regex_bio = regex_model.predict(test)\n",
    "\n",
    "regex_predictions = [\n",
    "    {\n",
    "        \"token\": data['tokens'],\n",
    "        \"labels\": extract_spans([l[1] for l in label])\n",
    "    }\n",
    "    for data, label in zip(test, regex_bio)\n",
    "]\n",
    "\n",
    "regex_score = evaluate_ner(test, regex_predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning model\n",
    "\n",
    "This is a transformers based model that is similar to the technologies behind LLMs/Generative AI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.flair_ner import flair_batch_predict, save_as_flair_fmt, load_flair_corpus, load_flair_ner, train_flair_ner\n",
    "\n",
    "# save the data in flair format\n",
    "save_as_flair_fmt(train, \"data/train.txt\")\n",
    "save_as_flair_fmt(test, \"data/test.txt\")\n",
    "\n",
    "corpus = load_flair_corpus(\"data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = \"models/flair-ner\"\n",
    "\n",
    "# Train the model\n",
    "# train_flair_ner(corpus, save_path, max_epochs=200, base_model=\"prajjwal1/bert-mini\")\n",
    "\n",
    "# get pre-trained model from Google Cloud Storage\n",
    "# !wget https://storage.googleapis.com/edwardqian-dev/workshop/best-model.pt -o f\"{save_path}/best-model.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flair_model = load_flair_ner(f\"{save_path}/best-model.pt\")\n",
    "\n",
    "from flair.data import Sentence\n",
    "\n",
    "test_sentences = [Sentence(\" \".join(d[\"tokens\"])) for d in test]\n",
    "flair_pred = flair_batch_predict(flair_model, test_sentences,  batch_size = 1)\n",
    "\n",
    "flair_score = evaluate_ner(test, flair_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative AI Model\n",
    "\n",
    "What if we use generative AI to solve this problem?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from src.gpt_ner import gpt_ner\n",
    "# gpt_bio = gpt_ner(str([\" \".join(d[\"tokens\"]) for d in test]), model=\"gpt-4-turbo\")\n",
    "\n",
    "\n",
    "gpt_bio = [['B-SVC', 'I-SVC', 'O', 'O', 'O', 'B-ENV'],\n",
    " ['B-SVC', 'I-SVC', 'B-ENV', 'I-ENV', 'I-ENV', 'O'],\n",
    " ['B-SVC', 'I-SVC', 'I-SVC', 'O', 'I-ENV', 'O'],\n",
    " ['B-SVC', 'I-SVC', 'O', 'O', 'O', 'O'],\n",
    " ['B-SVC', 'I-SVC', 'I-SVC', 'O', 'O', 'O'],\n",
    " ['B-SVC', 'I-SVC', 'I-SVC', 'I-SVC', 'O', 'O'],\n",
    " ['B-SVC', 'I-SVC', 'O', 'O', 'O', 'O'],\n",
    " ['B-SVC', 'I-SVC', 'I-SVC', 'I-SVC', 'O', 'O'],\n",
    " ['B-SVC', 'I-SVC', 'I-SVC', 'O', 'O', 'O'],\n",
    " ['B-SVC', 'I-SVC', 'I-SVC', 'O', 'O', 'O'],\n",
    " ['B-SVC', 'I-SVC', 'I-SVC', 'I-SVC', 'I-SVC', 'O']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_predictions = [\n",
    "    {\n",
    "        \"token\": data['tokens'],\n",
    "        \"labels\": extract_spans(label)\n",
    "    }\n",
    "    for data, label in zip(test, gpt_bio)\n",
    "]\n",
    "gpt_predictions\n",
    "\n",
    "gpt_score = evaluate_ner(test, gpt_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at all the results\n",
    "\n",
    "print(\"Random Model\")\n",
    "print(random_score)\n",
    "\n",
    "print(\"Regex Model\")\n",
    "print(regex_score)\n",
    "\n",
    "print(\"Flair Model\")\n",
    "print(flair_score)\n",
    "\n",
    "print(\"GPT Model\")\n",
    "print(gpt_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect the incorrect predictions\n",
    "\n",
    "Looking at scores are great, but they don't tell the whole story.\n",
    "\n",
    "It is ALWAYS a good idea to look at the actual data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at some sample incorrect predictions\n",
    "\n",
    "def print_sample_errors(true, pred, n=5):\n",
    "    for i, (t, p) in enumerate(zip(true, pred)):\n",
    "        t_spans = []\n",
    "        p_spans = []\n",
    "\n",
    "        for tl in t[\"labels\"]:\n",
    "            t_spans.append(f\"{' '.join(t['tokens'][tl[0]:tl[1]])} ({tl[2]})\")\n",
    "        for pl in p[\"labels\"]:\n",
    "            p_spans.append(f\"{' '.join(t['tokens'][pl[0]:pl[1]])} ({pl[2]})\")\n",
    "\n",
    "        if sorted(t_spans) != sorted(p_spans):\n",
    "            print(f\"Text: {' '.join(t['tokens'])}\")\n",
    "            print(f\"True: {t_spans}\")\n",
    "            print(f\"Pred: {p_spans}\")\n",
    "            print()\n",
    "            n -= 1\n",
    "        if n == 0:\n",
    "            break\n",
    "\n",
    "print(\"regex_model\")\n",
    "print_sample_errors(test, regex_predictions)\n",
    "print(\"flair_model\")\n",
    "print_sample_errors(test, flair_pred)\n",
    "print(\"gpt_model\")\n",
    "print_sample_errors(test, gpt_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample production data\n",
    "\n",
    "Our previous test data is randomly split from the training data.\n",
    "\n",
    "This data is good for development, but is not a true measure of data that the model has not seen before.\n",
    "\n",
    "Let's see what happens if we evaluate our model on a completely different dataset.\n",
    "\n",
    "This data helps us bridge the gap between offline and online predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prod_eval_labels = [\n",
    "    {\"tokens\": [\"Kube\", \"API\", \"Server\", \"Down\", \"in\", \"preprod\"],\n",
    "     \"labels\": [(0, 2, \"SVC\"), (5, 6, \"ENV\")]},\n",
    "    {\"tokens\": [\"Metrics\", \"Server\", \"Scaling\", \"Issues\", \"for\", \"test-na-4\"],\n",
    "     \"labels\": [(0, 2, \"SVC\"), (5, 6, \"ENV\")]},\n",
    "    {\"tokens\": [\"Redshift\", \"me-south-3\", \"Data\", \"Corruption\"],\n",
    "     \"labels\": [(0, 1, \"SVC\"), (1, 2, \"ENV\")]},\n",
    "    {\"tokens\": [\"Elasticsearch\", \"Cluster\", \"Overload\"],\n",
    "     \"labels\": [(0, 1, \"SVC\")]},\n",
    "    {\"tokens\": [\"Kafka\", \"Stream\", \"Delays\", \"in\", \"sandbox\", \"env\"],\n",
    "     \"labels\": [(0, 1, \"SVC\"), (4, 5, \"ENV\")]},\n",
    "    {\"tokens\": [\"Redis\", \"Cache\", \"Failure\", \"in\", \"prod-2,\", \"dev,\", \"and\", \"staging\"],\n",
    "     \"labels\": [(0, 2, \"SVC\"), (4, 5, \"ENV\"), (5, 6, \"ENV\"), (7, 8, \"ENV\")]},\n",
    "    {\"tokens\": [\"MongoDB\", \"Backup\", \"Failures\"], \"labels\": [(0, 1, \"SVC\")]},\n",
    "    {\"tokens\": [\"Nginx\", \"Proxy\", \"Timeout\"], \"labels\": [(0, 2, \"SVC\")]},\n",
    "    {\"tokens\": [\"Docker\", \"Registry\", \"Unreachable\"], \"labels\": [(0, 2, \"SVC\")]},\n",
    "    {\"tokens\": [\"GitLab\", \"CI\", \"Pipeline\", \"Stuck\"], \"labels\": [(0, 2, \"SVC\")]}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regex model\n",
    "regex_prod_eval_bio = regex_model.predict(prod_eval_labels)\n",
    "regex_prod_eval_predictions = [\n",
    "    {\n",
    "        \"token\": data['tokens'],\n",
    "        \"labels\": extract_spans([l[1] for l in label])\n",
    "    }\n",
    "    for data, label in zip(prod_eval_labels, regex_prod_eval_bio)\n",
    "]\n",
    "prod_eval_regex_score = evaluate_ner(prod_eval_labels, regex_prod_eval_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.data import Sentence\n",
    "prod_eval_flair_labels = [Sentence(\" \".join(d['tokens'])) for d in prod_eval_labels]\n",
    "prod_eval_flair_pred = flair_batch_predict(flair_model, prod_eval_flair_labels, batch_size=4)\n",
    "prod_eval_flair_score = evaluate_ner(prod_eval_labels, prod_eval_flair_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from src.gpt_ner import gpt_ner\n",
    "# prod_eval_gpt_bio = gpt_ner(str([\" \".join(d[\"tokens\"]) for d in prod_eval_labels]), model=\"gpt-4-turbo\")\n",
    "\n",
    "prod_eval_gpt_bio = [['B-SVC', 'I-SVC', 'I-SVC', 'O', 'O', 'B-ENV'],\n",
    " ['B-SVC', 'I-SVC', 'O', 'O', 'O', 'B-ENV'],\n",
    " ['B-SVC', 'B-ENV', 'O', 'O'],\n",
    " ['B-SVC', 'I-SVC', 'O'],\n",
    " ['B-SVC', 'I-SVC', 'O', 'O', 'B-ENV', 'I-ENV'],\n",
    " ['B-SVC', 'I-SVC', 'O', 'O', 'B-ENV', 'B-ENV', 'B-ENV', 'B-ENV'],\n",
    " ['B-SVC', 'O', 'O'],\n",
    " ['B-SVC', 'I-SVC', 'O'],\n",
    " ['B-SVC', 'I-SVC', 'O'],\n",
    " ['B-SVC', 'I-SVC', 'I-SVC', 'O']]\n",
    "\n",
    "prod_eval_gpt_predictions = [\n",
    "    {\n",
    "        \"token\": data['tokens'],\n",
    "        \"labels\": extract_spans(label)\n",
    "    }\n",
    "    for data, label in zip(prod_eval_labels, prod_eval_gpt_bio)\n",
    "]\n",
    "prod_eval_gpt_score = evaluate_ner(prod_eval_labels, prod_eval_gpt_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at some sample incorrect predictions\n",
    "\n",
    "print(\"regex_model\")\n",
    "print_sample_errors(prod_eval_labels, prod_eval_regex_score)\n",
    "print(\"flair_model\")\n",
    "print_sample_errors(prod_eval_labels, prod_eval_flair_pred)\n",
    "print(\"gpt4\")\n",
    "print_sample_errors(prod_eval_labels, prod_eval_gpt_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Selection\n",
    "## Deciding on the model\n",
    "\n",
    "1. Regex\n",
    "This approach is really simple, easy to scale, and can be integrated into any stack fairly easily (we can just port the regex into any language).\n",
    "\n",
    "Regex did well on the original test set but poorly on the out of distribution set.\n",
    "\n",
    "2. Flair\n",
    "This approach strongly prefers a python backend, we need to have specific scaffolding to scale and store the model.\n",
    "\n",
    "The flair model did a little better than regex on the original test set and much better than the regex on the out of distribution set (although still quite poor).\n",
    "\n",
    "3. GPT4\n",
    "This approach requires very actually training data, can be integrated into any stack since it's just a request.\n",
    "\n",
    "It's considerably more expensive per request and has much higher latency (prediction can take ~10 seconds).\n",
    "\n",
    "GPT4 did fairly reasonable for the out of distribution dataset but fairly poorly on the original test dataset.\n",
    "\n",
    "**Question**: what might be some reasons to choose one model over the other?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

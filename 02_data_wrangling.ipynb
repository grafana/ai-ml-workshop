{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"data/data.json\") as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the data\n",
    "\n",
    "text_sample = [\" \".join(d[\"tokens\"]) for d in data]\n",
    "print(f\"Text Sample: {text_sample[:5]}\")\n",
    "label_sample = [d[\"bio_tags\"] for d in data]\n",
    "print(f\"Label Sample: {label_sample[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- look at data breakdown\n",
    "- spliting into train and test\n",
    "- text data representation (simple embedding vs contextual embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see the distribution of the labels\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "label_counter = Counter()\n",
    "for labels in [d[\"bio_tags\"] for d in data]:\n",
    "    label_counter.update(labels)\n",
    "\n",
    "print(label_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at data with LOC tag\n",
    "\n",
    "[d for d in data if \"B-LOC\" in d[\"bio_tags\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are not enough LOC tags in the data, so we will drop them\n",
    "data = [d for d in data if \"B-LOC\" not in d[\"bio_tags\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import extract_spans\n",
    "\n",
    "# Let's see the distribution of the spans using a different data format\n",
    "\n",
    "for d in data:\n",
    "    spans = extract_spans(d[\"bio_tags\"])\n",
    "    d[\"labels\"] = spans\n",
    "\n",
    "\n",
    "data[:2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "span_counter = Counter()\n",
    "for d in data:\n",
    "    for span in d[\"labels\"]:\n",
    "        text = \" \".join(d[\"tokens\"][span[0]:span[1]]).lower()\n",
    "        span_counter.update([f\"{text}, {span[2]}\"])\n",
    "\n",
    "print(span_counter.most_common(5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Testing Sets in Machine Learning\n",
    "\n",
    "\n",
    "\n",
    "In machine learning, data is split into **training** and **testing sets** for two main reasons: model learning and model evaluation.\n",
    "\n",
    "\n",
    "\n",
    "#### **1. Training Set:**\n",
    "\n",
    "- **Purpose:** The training set is used to **train** or fit the model. It includes both the input data and the corresponding correct outputs.\n",
    "\n",
    "- **Why:** The model learns to recognize patterns and make predictions by adjusting its parameters based on the training data.\n",
    "\n",
    "\n",
    "\n",
    "#### **2. Testing Set:**\n",
    "\n",
    "- **Purpose:** The testing set is used to **evaluate** the performance of the model. It is separate from the training data and also includes the correct outputs.\n",
    "\n",
    "- **Why:** Using a separate testing set helps to assess how well the model can generalize to new, unseen data. This is crucial for understanding the model's effectiveness and ensuring it isn't just memorizing the training data (a problem known as overfitting).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "# save the split data\n",
    "with open(\"data/train.json\", \"w\") as f:\n",
    "    json.dump(train, f)\n",
    "with open(\"data/test.json\", \"w\") as f:\n",
    "    json.dump(test, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "\n",
    "Online vs offline metric:\n",
    "\n",
    "Online: to measure the AI/ML system performance in the real world\n",
    "\n",
    "Offline: to measure the AI/ML system performance using existing data while building the model.\n",
    "\n",
    "Challenge: These two don’t always match\n",
    "\n",
    "In offline evaluation, we can measure how “good” are the keywords we extract from our text.\n",
    "\n",
    "In online evaluation, we can measure success in two ways:\n",
    "- Implicit: user click through rate or adding suggestion to incident implies good labels\n",
    "- Explicit: suggest labels for user to accept/reject\n",
    "\n",
    "Good can be subjective:\n",
    "\n",
    "- Precision, recall, and f1 (small illustration in notebook)\n",
    "- High recall + low precision: costly false negative, e.g. fraud detection\n",
    "- High precision + low recall: costly false positive, e.g. legal\n",
    "- Balanced: both important\n",
    "\n",
    "**Question**:  What kind of mix do we want in this case?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Offline evaluation\n",
    "\n",
    "sample_text = \"google drive service high latency for prod us 1\"\n",
    "\n",
    "prediction_1 = [\n",
    "    (\"google drive\", \"SVC\"),  # correct\n",
    "    (\"prod us 1\", \"ENV\"),     # correct\n",
    "]\n",
    "# 2/2\n",
    "\n",
    "prediction_2 = [\n",
    "    (\"google drive\", \"SVC\"),  # correct\n",
    "    (\"prod us 1\", \"SVC\"),     # incorrect\n",
    "]\n",
    "# 1/2\n",
    "\n",
    "prediction_2 = [\n",
    "    (\"google drive\", \"SVC\"),  # correct\n",
    "]\n",
    "# 1/2\n",
    "\n",
    "prediction_3 = [\n",
    "    (\"service\", \"SVC\"),       # incorrect\n",
    "]\n",
    "# 0/2\n",
    "\n",
    "prediction_4 = [\n",
    "]\n",
    "# 0/2\n",
    "\n",
    "# A prediction is considered correct if both the span and the label are correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import evaluate_ner\n",
    "\n",
    "evaluate_ner(\n",
    "    [{\"tokens\": sample_text.split(), \"labels\": [(0, 2, 'SVC'), (6, 9, 'ENV')]}],\n",
    "    [{\"tokens\": sample_text.split(), \"labels\": [(0, 2, 'SVC'), (6, 9, 'SVC')]}],\n",
    ")\n",
    "# evaluate_ner(prediction_1, sample_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multisignal-rca-K6LZVNnX-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
